"""
ì¤‘ë³µ íŒŒì¼ ì‹ë³„ ëª¨ë“ˆ: SHA256 í•´ì‹±ì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¼ ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ íŒŒì¼ íƒì§€
"""

import hashlib
import os
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import fnmatch

from .config import OrganizerConfig


@dataclass
class FileInfo:
    """íŒŒì¼ ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    path: Path
    size: int
    hash: Optional[str] = None
    modified_time: float = 0.0
    created_time: float = 0.0

    def __post_init__(self):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì´ˆê¸°í™”"""
        if self.path.exists():
            stat = self.path.stat()
            self.size = stat.st_size
            self.modified_time = stat.st_mtime
            self.created_time = stat.st_ctime


@dataclass
class DuplicateGroup:
    """ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹"""
    hash: str
    files: List[FileInfo] = field(default_factory=list)
    total_size: int = 0

    def add_file(self, file_info: FileInfo):
        """íŒŒì¼ ì¶”ê°€"""
        self.files.append(file_info)
        self.total_size += file_info.size

    @property
    def wasted_space(self) -> int:
        """ë‚­ë¹„ë˜ëŠ” ê³µê°„ (ì›ë³¸ 1ê°œ ì œì™¸)"""
        if len(self.files) <= 1:
            return 0
        return self.total_size - self.files[0].size

    @property
    def count(self) -> int:
        """ì¤‘ë³µ íŒŒì¼ ìˆ˜"""
        return len(self.files)


class DuplicateFinder:
    """ì¤‘ë³µ íŒŒì¼ íƒì§€ í´ë˜ìŠ¤"""

    def __init__(self, config: OrganizerConfig):
        self.config = config
        self._size_groups: Dict[int, List[Path]] = defaultdict(list)
        self._hash_cache: Dict[Path, str] = {}

    def _should_exclude(self, path: Path) -> bool:
        """íŒŒì¼/í´ë” ì œì™¸ ì—¬ë¶€ í™•ì¸"""
        # í´ë” ì´ë¦„ ì²´í¬
        for part in path.parts:
            if part in self.config.excluded_dirs:
                return True

        # íŒŒì¼ íŒ¨í„´ ì²´í¬
        for pattern in self.config.excluded_patterns:
            if fnmatch.fnmatch(path.name, pattern):
                return True

        return False

    def _calculate_hash(self, file_path: Path, chunk_size: int = 65536) -> Optional[str]:
        """
        íŒŒì¼ì˜ SHA256 í•´ì‹œ ê³„ì‚°

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            chunk_size: ì½ê¸° ì²­í¬ í¬ê¸° (ê¸°ë³¸ 64KB)

        Returns:
            SHA256 í•´ì‹œ ë¬¸ìì—´ ë˜ëŠ” None (ì˜¤ë¥˜ ì‹œ)
        """
        # ìºì‹œ í™•ì¸
        if file_path in self._hash_cache:
            return self._hash_cache[file_path]

        try:
            sha256_hash = hashlib.sha256()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(chunk_size), b''):
                    sha256_hash.update(chunk)

            hash_value = sha256_hash.hexdigest()
            self._hash_cache[file_path] = hash_value
            return hash_value

        except (IOError, OSError, PermissionError) as e:
            return None

    def _calculate_partial_hash(self, file_path: Path, sample_size: int = 4096) -> Optional[str]:
        """
        íŒŒì¼ì˜ ë¶€ë¶„ í•´ì‹œ ê³„ì‚° (ë¹ ë¥¸ ì‚¬ì „ í•„í„°ë§ìš©)
        íŒŒì¼ì˜ ì‹œì‘, ì¤‘ê°„, ë ë¶€ë¶„ë§Œ í•´ì‹±

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            sample_size: ê° ë¶€ë¶„ì—ì„œ ì½ì„ ë°”ì´íŠ¸ ìˆ˜

        Returns:
            ë¶€ë¶„ í•´ì‹œ ë¬¸ìì—´ ë˜ëŠ” None
        """
        try:
            file_size = file_path.stat().st_size
            sha256_hash = hashlib.sha256()

            with open(file_path, 'rb') as f:
                # ì‹œì‘ ë¶€ë¶„
                sha256_hash.update(f.read(sample_size))

                if file_size > sample_size * 3:
                    # ì¤‘ê°„ ë¶€ë¶„
                    f.seek(file_size // 2)
                    sha256_hash.update(f.read(sample_size))

                    # ë ë¶€ë¶„
                    f.seek(-sample_size, 2)
                    sha256_hash.update(f.read(sample_size))

            return sha256_hash.hexdigest()

        except (IOError, OSError, PermissionError):
            return None

    def scan_directory(self, directory: Path) -> List[FileInfo]:
        """
        ë””ë ‰í† ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ìŠ¤ìº”í•˜ì—¬ íŒŒì¼ ì •ë³´ ìˆ˜ì§‘

        Args:
            directory: ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ

        Returns:
            FileInfo ê°ì²´ ë¦¬ìŠ¤íŠ¸
        """
        files = []

        try:
            for item in directory.rglob('*'):
                if item.is_file() and not self._should_exclude(item):
                    try:
                        size = item.stat().st_size
                        if size >= self.config.min_file_size:
                            files.append(FileInfo(path=item, size=size))
                    except (OSError, PermissionError):
                        continue
        except (OSError, PermissionError):
            pass

        return files

    def find_duplicates(self, directories: List[Path] = None) -> List[DuplicateGroup]:
        """
        ì§€ì •ëœ ë””ë ‰í† ë¦¬ë“¤ì—ì„œ ì¤‘ë³µ íŒŒì¼ íƒì§€

        Args:
            directories: ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)

        Returns:
            DuplicateGroup ë¦¬ìŠ¤íŠ¸
        """
        if directories is None:
            directories = self.config.target_directories

        # 1ë‹¨ê³„: ëª¨ë“  íŒŒì¼ ìŠ¤ìº” ë° í¬ê¸°ë³„ ê·¸ë£¹í™”
        print("ğŸ“ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        all_files: List[FileInfo] = []
        for directory in directories:
            if directory.exists() and directory.is_dir():
                all_files.extend(self.scan_directory(directory))

        print(f"   ì´ {len(all_files):,}ê°œ íŒŒì¼ ë°œê²¬")

        # í¬ê¸°ë³„ ê·¸ë£¹í™” (ë™ì¼ í¬ê¸° íŒŒì¼ë§Œ ì¤‘ë³µ í›„ë³´)
        size_groups: Dict[int, List[FileInfo]] = defaultdict(list)
        for file_info in all_files:
            size_groups[file_info.size].append(file_info)

        # í¬ê¸°ê°€ ê°™ì€ íŒŒì¼ì´ 2ê°œ ì´ìƒì¸ ê·¸ë£¹ë§Œ ì„ íƒ
        candidates = {size: files for size, files in size_groups.items() if len(files) > 1}
        candidate_count = sum(len(files) for files in candidates.values())
        print(f"   ì¤‘ë³µ í›„ë³´: {candidate_count:,}ê°œ íŒŒì¼ ({len(candidates):,}ê°œ í¬ê¸° ê·¸ë£¹)")

        # 2ë‹¨ê³„: ë¶€ë¶„ í•´ì‹œë¡œ ì¶”ê°€ í•„í„°ë§ (ëŒ€ìš©ëŸ‰ íŒŒì¼ ìµœì í™”)
        print("ğŸ” í•´ì‹œ ê³„ì‚° ì¤‘...")
        partial_hash_groups: Dict[Tuple[int, str], List[FileInfo]] = defaultdict(list)

        for size, files in candidates.items():
            for file_info in files:
                partial_hash = self._calculate_partial_hash(file_info.path)
                if partial_hash:
                    partial_hash_groups[(size, partial_hash)].append(file_info)

        # ë¶€ë¶„ í•´ì‹œë„ ê°™ì€ íŒŒì¼ë§Œ ì „ì²´ í•´ì‹œ ê³„ì‚°
        final_candidates = {k: v for k, v in partial_hash_groups.items() if len(v) > 1}

        # 3ë‹¨ê³„: ì „ì²´ í•´ì‹œ ê³„ì‚° ë° ìµœì¢… ì¤‘ë³µ ê·¸ë£¹ ìƒì„±
        hash_groups: Dict[str, DuplicateGroup] = {}
        total_to_hash = sum(len(files) for files in final_candidates.values())
        hashed_count = 0

        for (size, partial_hash), files in final_candidates.items():
            for file_info in files:
                full_hash = self._calculate_hash(file_info.path)
                if full_hash:
                    file_info.hash = full_hash
                    if full_hash not in hash_groups:
                        hash_groups[full_hash] = DuplicateGroup(hash=full_hash)
                    hash_groups[full_hash].add_file(file_info)

                hashed_count += 1
                if hashed_count % 100 == 0:
                    print(f"   ì§„í–‰: {hashed_count:,}/{total_to_hash:,}")

        # ì‹¤ì œ ì¤‘ë³µì¸ ê·¸ë£¹ë§Œ ë°˜í™˜ (2ê°œ ì´ìƒ íŒŒì¼)
        duplicates = [group for group in hash_groups.values() if group.count > 1]

        # ë‚­ë¹„ ê³µê°„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        duplicates.sort(key=lambda g: g.wasted_space, reverse=True)

        total_wasted = sum(g.wasted_space for g in duplicates)
        print(f"âœ… ì¤‘ë³µ ê·¸ë£¹ {len(duplicates):,}ê°œ ë°œê²¬")
        print(f"   ì ˆì•½ ê°€ëŠ¥í•œ ê³µê°„: {self._format_size(total_wasted)}")

        return duplicates

    def find_duplicates_parallel(self, directories: List[Path] = None,
                                  max_workers: int = 4) -> List[DuplicateGroup]:
        """
        ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¤‘ë³µ íŒŒì¼ íƒì§€ (ëŒ€ê·œëª¨ ë””ë ‰í† ë¦¬ìš©)

        Args:
            directories: ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ ë¦¬ìŠ¤íŠ¸
            max_workers: ìµœëŒ€ ì›Œì»¤ ìŠ¤ë ˆë“œ ìˆ˜

        Returns:
            DuplicateGroup ë¦¬ìŠ¤íŠ¸
        """
        if directories is None:
            directories = self.config.target_directories

        # íŒŒì¼ ìŠ¤ìº”
        print("ğŸ“ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        all_files: List[FileInfo] = []
        for directory in directories:
            if directory.exists() and directory.is_dir():
                all_files.extend(self.scan_directory(directory))

        print(f"   ì´ {len(all_files):,}ê°œ íŒŒì¼ ë°œê²¬")

        # í¬ê¸°ë³„ ê·¸ë£¹í™”
        size_groups: Dict[int, List[FileInfo]] = defaultdict(list)
        for file_info in all_files:
            size_groups[file_info.size].append(file_info)

        candidates = [(size, files) for size, files in size_groups.items() if len(files) > 1]
        print(f"   ì¤‘ë³µ í›„ë³´ ê·¸ë£¹: {len(candidates):,}ê°œ")

        # ë³‘ë ¬ í•´ì‹œ ê³„ì‚°
        print("ğŸ” ë³‘ë ¬ í•´ì‹œ ê³„ì‚° ì¤‘...")
        hash_groups: Dict[str, DuplicateGroup] = {}

        def process_file(file_info: FileInfo) -> Tuple[FileInfo, Optional[str]]:
            full_hash = self._calculate_hash(file_info.path)
            return file_info, full_hash

        files_to_hash = [f for _, files in candidates for f in files]

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {executor.submit(process_file, f): f for f in files_to_hash}
            completed = 0

            for future in as_completed(futures):
                file_info, full_hash = future.result()
                if full_hash:
                    file_info.hash = full_hash
                    if full_hash not in hash_groups:
                        hash_groups[full_hash] = DuplicateGroup(hash=full_hash)
                    hash_groups[full_hash].add_file(file_info)

                completed += 1
                if completed % 100 == 0:
                    print(f"   ì§„í–‰: {completed:,}/{len(files_to_hash):,}")

        duplicates = [group for group in hash_groups.values() if group.count > 1]
        duplicates.sort(key=lambda g: g.wasted_space, reverse=True)

        return duplicates

    @staticmethod
    def _format_size(size_bytes: int) -> str:
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.2f} PB"

    def get_summary(self, duplicates: List[DuplicateGroup]) -> Dict:
        """
        ì¤‘ë³µ íŒŒì¼ ë¶„ì„ ìš”ì•½ ì •ë³´ ë°˜í™˜

        Args:
            duplicates: ì¤‘ë³µ ê·¸ë£¹ ë¦¬ìŠ¤íŠ¸

        Returns:
            ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        total_groups = len(duplicates)
        total_files = sum(g.count for g in duplicates)
        total_wasted = sum(g.wasted_space for g in duplicates)
        total_size = sum(g.total_size for g in duplicates)

        return {
            "duplicate_groups": total_groups,
            "total_duplicate_files": total_files,
            "total_wasted_space": total_wasted,
            "total_wasted_space_formatted": self._format_size(total_wasted),
            "total_size": total_size,
            "total_size_formatted": self._format_size(total_size),
            "potential_files_to_remove": total_files - total_groups,
        }
